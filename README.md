# fake_news_rag
Syst√®me RAG pour la d√©tection de fake news 

## Structure projet (Mod√®le C4)
```mermaid

C4Component
title Architecture du projet RAG Fake News

Person(user, "Utilisateur", "Personne lan√ßant la d√©tection via le CLI ou main.py")

System_Boundary(rag_system, "RAG Fake News System") {

    Container(main, "main.py", "Script CLI principal", "Point d‚Äôentr√©e ‚Äî lance la d√©tection de fake news via le pipeline RAG")
    Container(cli, "cli.py", "Interface CLI (optionnelle)", "Fournit des commandes pour la d√©tection ou la g√©n√©ration de la base vectorielle")

    Container_Boundary(src, "src/") {
        Component(preprocessing, "preprocessing.py", "Nettoyage & pr√©paration des textes (tokenisation, lowercasing, etc.)")
        Component(embedding, "embedding.py", "Vectorisation (embeddings) des textes via Ollama ou autre mod√®le")
        Component(storage, "storage_chroma.py", "Stockage des embeddings dans une base vectorielle Chroma")
        Component(retrieval, "retrieval.py", "Recherche des documents similaires √† une requ√™te utilisateur")
        Component(rag_pipeline, "rag_pipeline.py", "Pipeline complet RAG (retrieval + LLM + r√©ponse)")
        Component(build_db, "build_vector_db.py", "Pipeline pour construire la base vectorielle √† partir de CSVs")
    }

    Container(data, "data/", "R√©pertoire de donn√©es", "Contient les fichiers bruts et trait√©s")
}

Rel(user, main, "Lance une d√©tection ou une commande CLI")
Rel(main, rag_pipeline, "Appelle la pipeline RAG")
Rel(rag_pipeline, retrieval, "Utilise pour r√©cup√©rer les contextes similaires")
Rel(retrieval, storage, "Interroge la base vectorielle Chroma")
Rel(build_db, preprocessing, "Charge et nettoie les donn√©es")
Rel(build_db, embedding, "Cr√©e les embeddings")
Rel(build_db, storage, "Ins√®re les embeddings dans Chroma")
Rel(cli, main, "Interface utilisateur alternative")

Rel(data, preprocessing, "Source des donn√©es (CSV bruts)")
Rel(storage, data, "Persiste les vecteurs et m√©tadonn√©es")

## Diagramme de s√©quence

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant CLI as Interface CLI / App
    participant PRE as Preprocessing
    participant CH as ChromaDB
    participant EMB as Embeddings (Ollama)
    participant LLM as LLM (Ollama)

    U->>CLI: Saisit une question
    CLI->>PRE: Nettoie et tokenise la requete
    PRE-->>CLI: Requete pretraitee

    CLI->>EMB: Genere un vecteur pour la requete
    EMB-->>CLI: Retourne le vecteur d embedding

    CLI->>CH: Recherche les documents similaires
    CH-->>CLI: Retourne les passages les plus pertinents

    CLI->>LLM: Envoie la question + contexte pertinent
    LLM-->>CLI: Genere une reponse contextuelle

    CLI-->>U: Affiche la reponse finale

```

## Diagramme de s√©quence

sequenceDiagram
    participant User as Utilisateur
    participant Main as main.py / cli.py
    participant RAG as rag_pipeline.py
    participant Retrieval as retrieval.py
    participant Storage as storage_chroma.py
    participant Model as Mod√®le LLM (Ollama)
    
    User->>Main: Saisit un texte ou lance une d√©tection
    Main->>RAG: Appelle RAGPipeline.detect(text)
    RAG->>Retrieval: R√©cup√®re les documents similaires
    Retrieval->>Storage: Interroge la base vectorielle (Chroma)
    Storage-->>Retrieval: Retourne les documents contextuels
    Retrieval-->>RAG: Renvoie le contexte pertinent
    RAG->>Model: Envoie la requ√™te + contexte pour r√©ponse
    Model-->>RAG: Retourne une pr√©diction (FAKE / R√âEL)
    RAG-->>Main: Renvoie le r√©sultat de la d√©tection
    Main-->>User: Affiche "üü• FAKE" ou "üü© R√âEL"


## Installation

### Cr√©er un environnement virtuel

```bash
cd ~/fake_news_rag
python3 -m venv .venv
source .venv/bin/activate
```

### Installer les d√©pendances

```bash
pip install -r requirements.txt
```
## Installer Ollama

### Installation

```bash
sudo snap install ollama

```

### V√©rification de l'installation

```bash
ollama list

```

### Installer un LLM

```bash
# Mod√®le pour l'embedding pour ChromaDB
ollama pull all-minilm
# Mod√®le plus 'gros'
ollama pull llama3.2
```

### V√©rifier que le mod√®le r√©ponde

```bash
cd ~/tests
python test_ollama.py
```

## Architecture du projet

```
rag-fake-news/
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                 # Donn√©es brutes
‚îÇ  ‚îÇ   ‚îú‚îÄ true.csv
‚îÇ  ‚îÇ   ‚îî‚îÄ fake.csv
‚îÇ  ‚îî‚îÄ processed/           # Donn√©es nettoy√©es pour l'embedding
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ preprocessing.py     # Module de traitements des donn√©es (chargement & nettoyage)
‚îÇ  ‚îú‚îÄ embedding.py         # Module de vectorisation des articles (embedding & normalisation)
‚îÇ  ‚îú‚îÄ storage_chroma.py    # Module de cr√©ation la base vectorielle (chargement & insertion)
‚îÇ  ‚îú‚îÄ retrieval.py         # Module mise en relation entre prompt utilisateur et base vectorielle 
‚îÇ  ‚îú‚îÄ rag_pipeline.py      # Pipeline RAG
‚îÇ  ‚îú‚îÄ build_vector_db.py   # Pipeline de cr√©ation de base vectorielle (traitement -> vectorisation -> insertion des donn√©es)
‚îÇ  ‚îî‚îÄ cli.py
‚îú‚îÄ tests/                    # Destin√© aux tests
‚îÇ  ‚îú‚îÄ test_preprocessing.py
‚îÇ  ‚îú‚îÄ test_ollama.py         # Permet de v√©rifier la liste des mod√®les install√©s et de v√©rifier leur fonctionnement
‚îÇ  ‚îú‚îÄ test_embedding.py
‚îÇ  ‚îú‚îÄ test_retrieval.py
‚îÇ  ‚îî‚îÄ ...
‚îú‚îÄ notebooks/
‚îú‚îÄ main.py                 # Script principal de lancement de d√©tection
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md

```
## Proc√©dures

### Traitement -> Vectorisation -> Cr√©ation & Insertion des donn√©es

La premi√®re √©tape consiste √† cr√©er un dossier data sous la forme :

‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                 # Donn√©es brutes
‚îÇ  ‚îÇ   ‚îú‚îÄ true.csv
‚îÇ  ‚îÇ   ‚îî‚îÄ fake.csv
‚îÇ  ‚îî‚îÄ processed/           # Dossiers accueillant les donn√©es trait√©es

Dans le dossier data/raw/, ins√©rer les csv True et Fake 
[Lien vers les datasets](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset/data)

Une fois le dossier data en place, il est d√©sormais possible de lancer le pipeline :

- Traitements des donn√©es -> Cr√©ation d'un dossier /processed dans /data
- Vectorisation 
- Cr√©ation de la base vectorielle avec ChromaDB
- Insertion des donn√©es avec cr√©ation d'une collection nomm√© "articles"

**Le processus de vectorisation est susceptible de prendre beaucoup temps (~1h) selon la puissance de votre machine.**

```bash
~/src
python build_vector_db.py

```

### Syst√®me RAG

```bash
~/fake_news_rag
python main.py

```

Au lancement du script depuis le terminal, l'utilisateur devra copier l'article.
-> Appuyez sur ENTREE pour confirmer le collage

Le syst√®me RAG se lance et va mettre en relation le prompt/article utilisateur avec la base vectorielle et retournera
un ensemble de 3 documents similaires.

Le corps de la r√©ponse est construit ainsi :

- Verdict : TRUE / FAKE 
- Reason : La raison qui explique le verdict
