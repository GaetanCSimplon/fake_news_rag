# fake_news_rag
Syst√®me RAG pour la d√©tection de fake news 

## Structure projet (Mod√®le C4)

```mermaid

C4Component
title Architecture du projet RAG Fake News

Person(user, "Utilisateur", "Personne lan√ßant la d√©tection via le CLI ou main.py")

System_Boundary(rag_system, "RAG Fake News System") {

    Container(main, "main.py", "Script CLI principal", "Point d'entree ‚Äî lance la detection de fake news via le pipeline RAG")
    Container(cli, "cli.py", "Interface CLI", "Fournit des commandes pour la detection ou la generation de la base vectorielle")

    Container_Boundary(src, "src/") {
        Component(preprocessing, "preprocessing.py", "Nettoyage et preparation des textes")
        Component(embedding, "embedding.py", "Vectorisation des textes via Ollama ou autre modele")
        Component(storage, "storage_chroma.py", "Stockage des embeddings dans une base vectorielle Chroma")
        Component(retrieval, "retrieval.py", "Recherche des documents similaires a une requete utilisateur")
        Component(rag_pipeline, "rag_pipeline.py", "Pipeline complet RAG (retrieval + LLM + reponse)")
        Component(build_db, "build_vector_db.py", "Pipeline pour construire la base vectorielle a partir de CSVs")
    }

    Container(data, "data/", "Repertoire de donnees", "Contient les fichiers bruts et traites")
}

Rel(user, main, "Lance une detection ou une commande CLI")
Rel(main, rag_pipeline, "Appelle la pipeline RAG")
Rel(rag_pipeline, retrieval, "Utilise pour recuperer les contextes similaires")
Rel(retrieval, storage, "Interroge la base vectorielle Chroma")
Rel(build_db, preprocessing, "Charge et nettoie les donnees")
Rel(build_db, embedding, "Cree les embeddings")
Rel(build_db, storage, "Insere les embeddings dans Chroma")
Rel(cli, main, "Interface utilisateur alternative")
Rel(data, preprocessing, "Source des donnees CSV")
Rel(storage, data, "Persiste les vecteurs et metadonnees")
```

## Diagramme de s√©quence
```mermaid
sequenceDiagram
    participant User as Utilisateur
    participant Main as main.py / cli.py
    participant RAG as rag_pipeline.py
    participant Retrieval as retrieval.py
    participant Storage as storage_chroma.py
    participant Model as Mod√®le LLM (Ollama)
    
    User->>Main: Saisit un texte ou lance une d√©tection
    Main->>RAG: Appelle RAGPipeline.detect(text)
    RAG->>Retrieval: R√©cup√®re les documents similaires
    Retrieval->>Storage: Interroge la base vectorielle (Chroma)
    Storage-->>Retrieval: Retourne les documents contextuels
    Retrieval-->>RAG: Renvoie le contexte pertinent
    RAG->>Model: Envoie la requ√™te + contexte pour r√©ponse
    Model-->>RAG: Retourne une pr√©diction (FAKE / R√âEL)
    RAG-->>Main: Renvoie le r√©sultat de la d√©tection
    Main-->>User: Affiche "üü• FAKE" ou "üü© R√âEL"
```

## Installation

### Cr√©er un environnement virtuel

```bash
cd ~/fake_news_rag
python3 -m venv .venv
source .venv/bin/activate
```

### Installer les d√©pendances

```bash
pip install -r requirements.txt
```
## Installer Ollama

### Installation

```bash
sudo snap install ollama

```

### V√©rification de l'installation

```bash
ollama list

```

### Installer un LLM

```bash
# Mod√®le pour l'embedding pour ChromaDB
ollama pull all-minilm
# Mod√®le plus 'gros'
ollama pull llama3.2
```

### V√©rifier que le mod√®le r√©ponde

```bash
cd ~/tests
python test_ollama.py
```

## Architecture du projet

```
rag-fake-news/
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                 # Donn√©es brutes
‚îÇ  ‚îÇ   ‚îú‚îÄ true.csv
‚îÇ  ‚îÇ   ‚îî‚îÄ fake.csv
‚îÇ  ‚îî‚îÄ processed/           # Donn√©es nettoy√©es pour l'embedding
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ preprocessing.py     # Module de traitements des donn√©es (chargement & nettoyage)
‚îÇ  ‚îú‚îÄ embedding.py         # Module de vectorisation des articles (embedding & normalisation)
‚îÇ  ‚îú‚îÄ storage_chroma.py    # Module de cr√©ation la base vectorielle (chargement & insertion)
‚îÇ  ‚îú‚îÄ retrieval.py         # Module mise en relation entre prompt utilisateur et base vectorielle 
‚îÇ  ‚îú‚îÄ rag_pipeline.py      # Pipeline RAG
‚îÇ  ‚îú‚îÄ build_vector_db.py   # Pipeline de cr√©ation de base vectorielle (traitement -> vectorisation -> insertion des donn√©es)
‚îÇ  ‚îî‚îÄ cli.py
‚îú‚îÄ tests/                    # Destin√© aux tests
‚îÇ  ‚îú‚îÄ test_preprocessing.py
‚îÇ  ‚îú‚îÄ test_ollama.py         # Permet de v√©rifier la liste des mod√®les install√©s et de v√©rifier leur fonctionnement
‚îÇ  ‚îú‚îÄ test_embedding.py
‚îÇ  ‚îú‚îÄ test_retrieval.py
‚îÇ  ‚îî‚îÄ ...
‚îú‚îÄ notebooks/
‚îú‚îÄ main.py                 # Script principal de lancement de d√©tection
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md

```
## Proc√©dures

### Traitement -> Vectorisation -> Cr√©ation & Insertion des donn√©es

La premi√®re √©tape consiste √† cr√©er un dossier data sous la forme :

‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                 # Donn√©es brutes
‚îÇ  ‚îÇ   ‚îú‚îÄ true.csv
‚îÇ  ‚îÇ   ‚îî‚îÄ fake.csv
‚îÇ  ‚îî‚îÄ processed/           # Dossiers accueillant les donn√©es trait√©es

Dans le dossier data/raw/, ins√©rer les csv True et Fake 
[Lien vers les datasets](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset/data)

Une fois le dossier data en place, il est d√©sormais possible de lancer le pipeline :

- Traitements des donn√©es -> Cr√©ation d'un dossier /processed dans /data
- Vectorisation 
- Cr√©ation de la base vectorielle avec ChromaDB
- Insertion des donn√©es avec cr√©ation d'une collection nomm√© "articles"

**Le processus de vectorisation est susceptible de prendre beaucoup temps (~1h) selon la puissance de votre machine.**

```bash
~/src
python build_vector_db.py

```

### Syst√®me RAG

```bash
~/fake_news_rag
python main.py

```

Au lancement du script depuis le terminal, l'utilisateur devra copier l'article.
-> Appuyez sur ENTREE pour confirmer le collage

Le syst√®me RAG se lance et va mettre en relation le prompt/article utilisateur avec la base vectorielle et retournera
un ensemble de 3 documents similaires.

Le corps de la r√©ponse est construit ainsi :

- Verdict : TRUE / FAKE 
- Reason : La raison qui explique le verdict
